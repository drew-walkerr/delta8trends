---
title: "Analytic Code for #Delta8 Tweets"
author: "Drew Walker"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
bibliography: references.bib
---

# Tidy Adaptation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#remotes::install_github("slu-openGIS/postmastr")
library(table1)
library(operators)
library(magrittr)
library(stm)
library(here)
library(textmineR)
library(tidyverse)
library(tidytext)
library(knitr) #Create nicely formatted output tables
library(kableExtra) #Create nicely formatted output tables
library(formattable) #For the color_tile function
library(lubridate)
library(academictwitteR)
library(tm)
library(tidygeocoder)
library(ggpubr)
library(topicmodels)
library(scales)
library(purrr)
library(gt)
library(webshot)
library(sentimentr)
library(syuzhet)
library(gitcreds)
library(textdata)
library(credentials)
library(patchwork)
library(quanteda)
library(quanteda.dictionaries)
library(sjPlot)
library(sjmisc)
library(patchwork)
library(sjlabelled)

#devtools::install_github("kbenoit/quanteda.dictionaries")
APIToken <- read_csv("apikeys.csv")
api_key <- as.character(APIToken[1,1])
api_secret_key <- as.character(APIToken[1,2])
bearer_token <- as.character(APIToken[1,3])
```


```{r get-hashtag-delta8-tweets}
#
##Get tweets with Delta 8 thc or delta 8 hashtag

today <- as.POSIXct(Sys.Date())

```

# Tweet data NLP and trend analysis

```{r delta8tweets}
d8_tweets <- readRDS("hashtag_delta8_tweets_and_user_info_jan_2020.rds")

d8_tweets_raw <- readRDS("hashtag_delta8_tweets_and_user_info_jan_2020.rds")
```

# prepping for nlp


* Word count included we removed RT, @ to filter out user handles, digits, urls, and stop words and 1-character words. 

* For NRC sentiment analyses, we removed RT, @ to filter out user handles, digits, urls, and stop words and 1-character words

* For szuzyhnet sentence-level sentiment analysis, which uses sentence-level aggregates and incorporates weighting of negating terms that have been demonstrated to significantly affect accuracy at predicting sentence-level sentiment. 

```{r, preprocessing-tweets}
#Remove @ and RT tweet notation
d8_tweets$text <- gsub("RT.*:", "", d8_tweets$text)
d8_tweets$text <- gsub("@.* ", "", d8_tweets$text)
# sub out digits , punctuation
#Should digits be included in case of delta 8, delta 10? 
d8_tweets_text_id <- d8_tweets %>% 
   select(id,text)


d8_tweets$text <- gsub('[[:punct:]]+', '', d8_tweets$text)
text_cleaning_tokens <- d8_tweets %>% 
  tidytext::unnest_tokens(word, text) %>% 
  left_join(d8_tweets_text_id, by = "id") %>%
  mutate(raw_text = text)
#remove words? like 


text_cleaning_tokens$word <- gsub('^https|amp','', text_cleaning_tokens$word)

text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)

#count words
word_count <- text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word) %>% 
  arrange(-n) %>% 
  slice_max(n, n = 30) %>% 
  filter(word != "delta8")

mergeable_word_count <- text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word)


head(word_count,
     n = 20)

word_count_bar <- ggplot(word_count, aes(x= fct_reorder(word,n),y = n, fill = word,label=n)) + 
          geom_col(show.legend = FALSE, position = position_dodge(width = 1))+
  labs(x = "Word", y = "Count in Delta 8 Tweets")+
  geom_text(size=3,nudge_x = .2,nudge_y = 1500)+
  coord_flip()+
  theme_pubclean()
  
word_count_bar

current_time <- Sys.time()
st <- format(current_time,"%Y-%m-%d_%H_%M")
word_count_bar_filename <- paste0(here("word_count"),st,".png") 

ggsave(word_count_bar_filename)

```


# Sentiment Analysis

* NRC lexicon used

  * break into by word and merge by sentiment 


```{r nrc, eval = FALSE}
library(furrr)
#using furrr for parallel processing, help to prevent crashing w/possibly
possible_nrc_sentiment <- possibly(get_nrc_sentiment,otherwise = tibble("NA"))

nrc_sentiment_d8 <- d8_tweets %>% 
  mutate(nrc_sentiment_df = future_map(text, possible_nrc_sentiment),
    corpus = "delta8") %>% 
      unnest()
    
nrc_sentiment_d8_by_author <- nrc_sentiment_d8 %>% 
  group_by(author_id) %>% 
  summarize(corpus= corpus,
            number_of_tweets = n(),
            anger = mean(anger),
            anticipation = mean(anticipation),
            disgust = mean(disgust),
            fear = mean(fear),
            joy = mean(joy),
            sadness = mean(sadness),
            surprise = mean(surprise),
            trust = mean(trust),
            negative = mean(negative),
            positive = mean(positive)) %>% 
  distinct(author_id, .keep_all = TRUE) %>% 
  group_by(author_id) %>% 
  mutate(percentage_of_tweets = number_of_tweets/41828)
mean(nrc_sentiment_d8_by_author$number_of_tweets)

nrc <- get_sentiments("nrc")
nrc_words <- mergeable_word_count %>%
  left_join(nrc, by="word")
# Top words sentiment
top_30_word_sentiment <- word_count %>%
  left_join(nrc, by="word")
# only top word was enjoy 

nrc_sentiment_top_terms <- nrc_words %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5) %>% 
  mutate(top_terms = paste0(word, collapse = " , ")) %>% 
  ungroup() %>%
  arrange(sentiment, -n) 



kable(nrc_sentiment_top_terms)

bar_words<- nrc_words %>%
  group_by(sentiment) %>% # group by sentiment type
  summarize(word_count = sum(n)) %>% # counts number of rows
  arrange(desc(word_count)) %>% 
  mutate(percent = word_count/sum(word_count)) %>% 
  left_join(nrc_sentiment_top_terms, by = "sentiment")
       
bar_words_table <- bar_words %>% 
  select(sentiment,word_count,percent,top_terms) %>% 
  rename(word_count_by_sentiment = word_count) %>% 
  distinct(sentiment,word_count_by_sentiment,percent,top_terms)

bar_words_table_gt <- gt(bar_words_table)

bar_words_table_gt
bar_words_table_gt %>% 
  gtsave("nrc_sentiment_table.html", inline_css = TRUE)

distinct_nrc <- distinct(nrc_words,word)
```

```{r, trends-over-time}

d8_tweets_by_date <- d8_tweets %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day)) %>% 
  dplyr::group_by(day) %>% 
  dplyr::summarize(tweets_per_day = n(),
            likes_per_day = sum(public_metrics.x$like_count, na.rm = TRUE),
            retweets_per_day = sum(public_metrics.x$retweet_count, na.rm = TRUE),
            replies_per_day = sum(public_metrics.x$reply_count, na.rm = TRUE),
            quotes_per_day = sum(public_metrics.x$quote_count, na.rm = TRUE)) %>% 
  mutate(month = month(day),
         year = year(day)) %>% 
  mutate(`Tweet Source` = "All Accounts")
# Major accounts posts
major_accounts_by_date <- d8_tweets %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day)) %>% 
  dplyr::group_by(day) %>% 
  filter(author_id == "1294976643241476099" | author_id == "1386842121102143490") %>%  
  dplyr::summarize(tweets_per_day = n(),
           likes_per_day = sum(public_metrics.x$like_count, na.rm = TRUE),
           retweets_per_day = sum(public_metrics.x$retweet_count, na.rm = TRUE),
           replies_per_day = sum(public_metrics.x$reply_count, na.rm = TRUE),
           quotes_per_day = sum(public_metrics.x$quote_count, na.rm = TRUE)) %>% 
  mutate(month = month(day),
         year = year(day)) %>% 
  mutate(`Tweet Source` = "Top Two Accounts")

d8_tweets_by_date_with_major_accounts <- rbind(d8_tweets_by_date,major_accounts_by_date)

tweets_per_day <- ggplot(d8_tweets_by_date, aes(day, tweets_per_day)) +
  geom_line(aes(y = tweets_per_day))+
  labs(y = "Tweets per Day", title = "#Delta8 Tweets")+
  scale_x_date(limits = c(as.Date("2020-01-01"), as.Date("2021-09-26")), breaks = as.Date(c("2020-01-01","2021-09-26","2020-11-12"), date_labels = "%b-%Y"))+    theme_classic()+theme(axis.title.x=element_blank())
tweets_per_day
tweets_per_day_filename <- paste0(here("tweets_per_day"),st,".png") 
ggsave(tweets_per_day_filename)

#likes per day graph
likes_per_day <- ggplot(d8_tweets_by_date, aes(day, likes_per_day)) +
  geom_line(aes(y = likes_per_day))+
  labs(y = "Likes per Day", title = "#Delta8 Tweet Likes")+
  scale_x_date(limits = c(as.Date("2020-01-01"), as.Date("2021-09-26")), breaks = as.Date(c("2020-01-01","2021-09-26","2020-11-12"), date_labels = "%b-%Y"))+    theme_classic()+
  theme(axis.title.x=element_blank())
likes_per_day
likes_per_day_file_name <- paste0(here("likes_per_day"),st,".png") 
ggsave(likes_per_day_file_name)

#rerunning retweets,quotes, and replies 

retweets <- d8_tweets_raw %>% 
  filter(referenced_tweets != "NULL")


retweet_d8_tweets_by_date <- retweets %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day)) %>% 
  dplyr::group_by(day) %>% 
  dplyr::summarize(tweets_per_day = n()) %>% 
  mutate(month = month(day),
         year = year(day))
retweets_per_day <- ggplot(retweet_d8_tweets_by_date, aes(day, retweets_per_day)) +
  geom_line(aes(y = tweets_per_day))+
  labs(y = "RTs per Day", title = "#Delta8 Retweets per day")+
  scale_x_date(limits = c(as.Date("2020-01-01"), as.Date("2021-09-26")), breaks = as.Date(c("2020-01-01","2021-09-26","2020-11-12"), date_labels = "%b-%Y"))+    theme_classic()+ 
  theme(axis.title.x=element_blank())
retweets_per_day
retweets_per_day_file_name <- paste0(here("retweets_per_day"),st,".png") 
ggsave(retweets_per_day_file_name)



quotes_per_day <- ggplot(d8_tweets_by_date, aes(day, quotes_per_day)) +
  geom_line(aes(y = quotes_per_day))+
  labs(y = "Quotes per Day", title = "#Delta8 Quotes")+
  scale_x_date(limits = c(as.Date("2020-01-01"), as.Date("2021-09-26")), breaks = as.Date(c("2020-01-01","2021-09-26","2020-11-12"), date_labels = "%b-%Y"))+
    theme_classic() + 
  theme(axis.title.x = element_blank())
quotes_per_day
quotes_per_day_file_name <- paste0(here("quotes_per_day"),st,".png") 
ggsave(quotes_per_day_file_name)

replies_per_day <- ggplot(d8_tweets_by_date, aes(day, replies_per_day)) +
  geom_line(aes(y = replies_per_day))+
  labs(y = "Replies per Day", title = "#Delta8 Tweet Replies")+
    scale_x_date(limits = c(as.Date("2020-01-01"), as.Date("2021-09-26")), breaks = as.Date(c("2020-01-01","2021-09-26","2020-11-12"), date_labels = "%b-%Y"))+
    theme_classic()+
  theme(axis.title.x=element_blank())
replies_per_day
replies_per_day_file_name <- paste0(here("replies_per_day"),st,".png") 
ggsave(replies_per_day_file_name)

all_graphs <- tweets_per_day / likes_per_day / retweets_per_day / quotes_per_day + plot_layout(widths = c(4, 1))

all_graphs

all_graphs_filename <- paste0(here("trend_graph_collage"),st,".png") 
ggsave(all_graphs_filename,width = 10, height = 10)
library(table1)
table1(~ tweets_per_day|factor(year), data=d8_tweets_by_date)

```


#Unique tweet by text
```{r unique-tweet-text}
unique_text_d8_tweets <- d8_tweets %>% 
  distinct(text)
```

# Random 25 tweets from each of the top terms identified
https://stat.ethz.ch/R-manual/R-devel/library/base/html/sample.html

```{r random-qualitative-tweets}

delta_thc <- d8_tweets %>% 
  filter(str_detect(text,"deltathc"))

set.seed(424242)
random_tweets <- function(word){
random_df <- d8_tweets_raw %>% 
  filter(str_detect(text,word)) %>% 
  slice_sample(n=10) %>% 
  select(id,text,name) %>% 
  mutate(retailer= "",
         delta_8_post = "",
         therapeutic_claim = "",
         effects_mentioned = "")
return(random_df)
}


library(purrr)
library(furrr)
possible_none <- purrr::possibly(random_tweets, otherwise = tidyr::tibble("NA"))

words_dfs <- word_count %>% 
  mutate(sample_tweets = future_map(word_count$word, possible_none)) %>% 
  unnest(sample_tweets)
write_csv(words_dfs,"top_words_d8_sample_tweets.csv")


words_dfs %>% 
  group_by(word) %>% 
  summarize(count= n())
         
         


```
# Retweet investigating
There was a tweet by YoKratom posted from multiple users, all with same value of Retweets (492)
```{r retweet-investigate}
retweeted_check <- d8_tweets %>% 
  filter(public_metrics.x$retweet_count == 492) 
users <- retweeted_check %>% 
  distinct(author_id, .keep_all = TRUE)
  
original_tweets <- d8_tweets_raw %>% 
  filter(referenced_tweets == "NULL")
#30826 original tweets
#retweets
retweets <- d8_tweets_raw %>% 
  filter(referenced_tweets != "NULL")

```

```{r authors}
author_list <- d8_tweets %>% 
  distinct(author_id, .keep_all = TRUE)
```

## Revisions code

### Tweet Normalization
* Compare frequencies of tweet 
```{r normalization}
users_per_year_total <- read_csv("estimated_users_per_year.csv")

users_per_year_total_clean <- users_per_year_total %>% 
  mutate(total_twitter_users = number_of_twitter_users_in_millions*1000000)

d8_users_per_year <- d8_tweets %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day)) %>% 
  group_by(year) %>% 
  summarize(users_per_year = n_distinct(author_id))

normalized_users_per_year_df <- left_join(d8_users_per_year,users_per_year_total_clean, by = "year")

normalized_user_df_clean <- normalized_users_per_year_df %>% 
  mutate(d8_users_per_hund_thoussand = (users_per_year/total_twitter_users)*100000)
normalized_user_df_clean
```

## Control group df 
Make control group using query that is only stop words, compare sentiment analysis frequencies 
```{r control-df, eval = FALSE}
APIToken <- read_csv("apikeys.csv")
api_key <- as.character(APIToken[1,1])
api_secret_key <- as.character(APIToken[1,2])
bearer_token <- as.character(APIToken[1,3])

snowball <- stop_words[stop_words$lexicon == "snowball",]
stopwords_list <- as.list(snowball$word)

#Since query character limit is 1600 or so, we have to make a for loop for all of the words or figure out how 

stop_word_tweets <- function(stopword){ 
stop_word_tweets_df <- get_all_tweets(
  query = stopword,
  bearer_token = bearer_token,
  start_tweets = "2020-01-01T00:00:00Z",
  end_tweets = "2021-09-26T00:00:00Z",
  data_path = "control_data/",
  n = 1000)
return(stop_word_tweets_df)
}
library(purrr)
library(furrr)
possible_stop_word_tweets <- possibly(stop_word_tweets,otherwise = tibble("NA"))

stop_words_tweet_dfs <- snowball %>% 
  mutate(stop_word_df = future_map(word,possible_stop_word_tweets)) 

saveRDS(stop_words_tweet_dfs, file= "random_stopword_tweets.rds")
```

```{r load-control}
random_stopword_tweets <- readRDS("random_stopword_tweets.rds")
flattened_df <- unnest(random_stopword_tweets)

word_counts = flattened_df %>% 
  group_by(word) %>% 
  summarise(cound = n())

set.seed(12345)

random_df <- flattened_df %>% 
  slice_sample(n=41828)
``` 


```{r control-df-preprocessing}
random_df$text <- gsub("RT.*:", "", random_df$text)
random_df$text <- gsub("@.* ", "", random_df$text)
# sub out digits , punctuation
#Should digits be included in case of delta 8, delta 10? 
random_df_text_id <- random_df %>% 
   select(id,text)
random_df$text <- gsub('[[:punct:]]+', '', random_df$text)
random_text_cleaning_tokens <- random_df %>% 
  tidytext::unnest_tokens(word, text) %>% 
  left_join(random_df_text_id, by = "id") %>%
  mutate(raw_text = text)
#remove words? like 
random_text_cleaning_tokens$word <- gsub('^https|amp','', random_text_cleaning_tokens$word)
random_text_cleaning_tokens <- random_text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)
```


```{r control-df-sentiment-analysis}
# Mergeable wordcount 
random_word_count <- random_text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word) %>% 
  arrange(-n) %>% 
  slice_max(n, n = 30) %>% 
  filter(word != "delta8")
random_mergeable_word_count <- random_text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word)


# RANDOM SENTIMENT ANALYSIS

sentiment_random_df <- random_df %>%  
  mutate(nrc_sentiment_df = future_map(text, possible_nrc_sentiment),
    corpus = "control") %>% 
      unnest()

nrc_sentiment_control_by_author <- sentiment_random_df %>% 
  group_by(author_id) %>% 
  summarize(corpus= corpus,
            number_of_tweets = n(),
            anger = mean(anger),
            anticipation = mean(anticipation),
            disgust = mean(disgust),
            fear = mean(fear),
            joy = mean(joy),
            sadness = mean(sadness),
            surprise = mean(surprise),
            trust = mean(trust),
            negative = mean(negative),
            positive = mean(positive)) %>% 
  distinct(author_id, .keep_all = TRUE) %>% 
  group_by(author_id) %>% 
  mutate(percentage_of_tweets = number_of_tweets/41828)
mean(nrc_sentiment_control_by_author$number_of_tweets)

nrc <- get_sentiments("nrc")
random_nrc_words <- random_mergeable_word_count %>%
  left_join(nrc, by="word")



random_nrc_sentiment_top_terms <- random_nrc_words %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5) %>% 
  mutate(top_terms = paste0(word, collapse = " , ")) %>% 
  ungroup() %>%
  arrange(sentiment, -n) 
kable(random_nrc_sentiment_top_terms)


random_bar_words<- random_nrc_words %>%
  group_by(sentiment) %>% # group by sentiment type
  summarize(word_count = sum(n)) %>% # counts number of rows
  arrange(desc(word_count)) %>% 
  mutate(percent = word_count/sum(word_count)) %>% 
  left_join(random_nrc_sentiment_top_terms, by = "sentiment")

random_bar_words_table <- random_bar_words %>% 
  select(sentiment,word_count,percent,top_terms) %>% 
  rename(word_count_by_sentiment = word_count) %>% 
  distinct(sentiment,word_count_by_sentiment,percent,top_terms)


random_bar_words_table_gt <- gt(random_bar_words_table)
random_bar_words_table_gt

random_bar_words_table_gt %>% 
  gtsave("random_nrc_sentiment_table.html", inline_css = TRUE)
```

## next steps

* Join random sentiment dataframe with delta 8

```{r combined-control-and-d8}
nrc_sentiment_d8_by_author

combined_sentiment_df <- bind_rows(nrc_sentiment_control_by_author,nrc_sentiment_d8_by_author)
saveRDS(combined_sentiment_df, file= "combined_author_sentiment_df.rds")
```

Compare % differences among each 
```{r}
combined_sentiment_df <- readRDS("combined_author_sentiment_df.rds")

anger = lm(anger~corpus, data = combined_sentiment_df)
summary(anger)
anticipation = lm(anticipation~corpus, data = combined_sentiment_df)
summary(anticipation)
disgust = lm(disgust~corpus, data = combined_sentiment_df)
summary(disgust)

fear = lm(fear~corpus, data = combined_sentiment_df)
summary(fear)

joy = lm(joy~corpus, data = combined_sentiment_df)
summary(joy)

sadness = lm(sadness~corpus, data = combined_sentiment_df)
summary(sadness)

surprise = lm(surprise~corpus, data = combined_sentiment_df)
summary(surprise)

trust = lm(trust~corpus, data = combined_sentiment_df)
summary(trust)

negative = lm(negative~corpus, data = combined_sentiment_df)
summary(negative)

positive = lm(positive~corpus, data = combined_sentiment_df)
summary(positive)

tab_model(anger,anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive, dv.labels = c("Anger", "Anticipation","Disgust","Fear", "Joy", "Sadness", "Surprise", "Trust", "Negative", "Positive"), show.aic = TRUE, show.dev = TRUE)


```

