---
title: "delta8tweets"
author: "Drew Walker"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
bibliography: references.bib
---

# Tidy Adaptation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#remotes::install_github("slu-openGIS/postmastr")
library(table1)
library(operators)
library(magrittr)
library(stm)
library(here)
library(postmastr)
library(textmineR)
library(tidyverse)
library(tidytext)
library(knitr) #Create nicely formatted output tables
library(kableExtra) #Create nicely formatted output tables
library(formattable) #For the color_tile function
library(lubridate)
library(academictwitteR)
library(tm)
library(tidygeocoder)
library(ggpubr)
library(topicmodels)
library(scales)
library(gt)
library(webshot)
library(sentimentr)
library(syuzhet)
library(gitcreds)
library(credentials)


APIToken <- read_csv("apikeys.csv")
api_key <- as.character(APIToken[1,1])
api_secret_key <- as.character(APIToken[1,2])
bearer_token <- as.character(APIToken[1,3])
```

# Resources for topic modeling

Tidy Text Mining With R (Julia Silge) <https://www.tidytextmining.com/topicmodeling.html>

Predominantly used: has some older code that i had to fix, may try to convert to Julia's topicmodels package <https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25>

# Pull tweet full archive tweet data

-   Add search by hashtag

```{r get-hashtag-delta8-tweets}
#
##Get tweets with Delta 8 thc or delta 8 hashtag

today <- as.POSIXct(Sys.Date())
hashtag_delta8_tweets_jan_2020 <-  get_all_tweets(
  query = "#delta8",
  start_tweets = "2020-01-01T00:00:00Z",
  end_tweets = "2021-08-20T00:00:00Z",
  bearer_token,data_path = "data/",
  n = 100000)

users_hashtag_d8_tweets_jan_2020 <-
get_user_profile(unique(hashtag_delta8_tweets_jan_2020$author_id), bearer_token)
#
users_hashtag_d8_tweets_jan_2020 <- users_hashtag_d8_tweets_jan_2020 %>% 
  rename(author_id = id)
#
#
hashtag_delta8_tweets_and_user_info_jan_2020 <- left_join(hashtag_delta8_tweets_jan_2020, users_hashtag_d8_tweets_jan_2020, by = "author_id")

write_rds(hashtag_delta8_tweets_and_user_info_jan_2020, "hashtag_delta8_tweets_and_user_info_jan_2020.rds")
```

# Tweet data NLP and trend analysis

```{r delta8tweets}
d8_tweets <- readRDS("hashtag_delta8_tweets_and_user_info_jan_2020.rds")
```

# prepping for nlp


* Word count included we removed RT, @ to filter out user handles, digits, urls, and stop words and 1-character words. 

* For NRC sentiment analyses, we removed RT, @ to filter out user handles, digits, urls, and stop words and 1-character words

* For szuzyhnet sentence-level sentiment analysis, which uses sentence-level aggregates and incorporates weighting of negating terms that have been demonstrated to significantly affect accuracy at predicting sentence-level sentiment. 

```{r, preprocessing-tweets}
#Remove @ and RT tweet notation
d8_tweets$text <- gsub("RT.*:", "", d8_tweets$text)
d8_tweets$text <- gsub("@.* ", "", d8_tweets$text)
# sub out digits , punctuation
#Should digits be included in case of delta 8, delta 10? 
d8_tweets_text_id <- d8_tweets %>% 
   select(id,text)


d8_tweets$text <- gsub('[[:punct:]]+', '', d8_tweets$text)
text_cleaning_tokens <- d8_tweets %>% 
  tidytext::unnest_tokens(word, text) %>% 
  left_join(d8_tweets_text_id, by = "id") %>%
  mutate(raw_text = text)
#remove words? like 

text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('^https|amp','', text_cleaning_tokens$word)

text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)

#count words
word_count <- text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word) %>% 
  arrange(-n) %>% 
  slice_max(n, n = 20)

mergeable_word_count <- text_cleaning_tokens %>% 
  filter(word != "") %>% 
  count(word)


head(word_count,
     n = 20)

word_count_bar <- ggplot(word_count, aes(x= fct_reorder(word,n),y = n, fill = word)) + 
          geom_col(show.legend = FALSE)+
  labs(x = "Word", y = "Count in Delta 8 Tweets")+
  coord_flip()+
  theme_pubclean()
word_count_bar

current_time <- Sys.time()
st <- format(current_time,"%Y-%m-%d_%H_%M")
word_count_bar_filename <- paste0(here("word_count"),st,".png") 

ggsave(word_count_bar_filename)

```


# Sentiment Analysis

* NRC lexicon used

  * break into by word and merge by sentiment 


```{r, nrc}
nrc <- get_sentiments("nrc") # get specific sentiment lexicons in a tidy format

nrc_words <- mergeable_word_count %>%
  inner_join(nrc, by="word")

nrc_sentiment_top_terms <- nrc_words %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5) %>% 
  mutate(top_terms = paste0(word, collapse = " , ")) %>% 
  ungroup() %>%
  arrange(sentiment, -n) 



kable(nrc_sentiment_top_terms)

bar_words<- nrc_words %>%
  group_by(sentiment) %>% # group by sentiment type
  tally() %>% # counts number of rows
  arrange(desc(n)) %>% 
  mutate(percent = n/sum(n)) %>% 
  left_join(nrc_sentiment_top_terms, by = "sentiment")
       
bar_words_table <- bar_words %>% 
  select(sentiment,n.x,percent,top_terms) %>% 
  rename(word_count_by_sentiment = n.x) %>% 
  distinct(sentiment,word_count_by_sentiment,percent,top_terms)

bar_words_table_gt <- gt(bar_words_table)

bar_words_table_gt
bar_words_table_gt %>% 
  gtsave("nrc_sentiment_table.html", inline_css = TRUE)

```

```{r, trends-over-time}

d8_tweets_by_date <- d8_tweets %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day)) %>% 
  dplyr::group_by(day) %>% 
  dplyr::summarize(tweets_per_day = n(),
            likes_per_day = sum(public_metrics.x$like_count, na.rm = TRUE),
            retweets_per_day = sum(public_metrics.x$retweet_count, na.rm = TRUE),
            replies_per_day = sum(public_metrics.x$reply_count, na.rm = TRUE),
            quotes_per_day = sum(public_metrics.x$quote_count, na.rm = TRUE))

tweets_per_day <- ggplot(d8_tweets_by_date, aes(day, tweets_per_day)) +
  geom_line(aes(y = tweets_per_day), color = "darkred")+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "Tweets per Day", title = "Frequency of Original Tweets per day mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
tweets_per_day
tweets_per_day_filename <- paste0(here("tweets_per_day"),st,".png") 
ggsave(tweets_per_day_filename)

#likes per day graph
likes_per_day <- ggplot(d8_tweets_by_date, aes(day, likes_per_day)) +
  geom_line(aes(y = likes_per_day), color = "darkblue")+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "Likes per Day", title = "Frequency of Tweet likes mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
likes_per_day
likes_per_day_file_name <- paste0(here("likes_per_day"),st,".png") 
ggsave(likes_per_day_file_name)

retweets_per_day <- ggplot(d8_tweets_by_date, aes(day, retweets_per_day)) +
  geom_line(aes(y = retweets_per_day), color = "orange")+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "Retweets per Day", title = "Frequency of Tweet retweets mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
retweets_per_day
retweets_per_day_file_name <- paste0(here("retweets_per_day"),st,".png") 
ggsave(retweets_per_day_file_name)

quotes_per_day <- ggplot(d8_tweets_by_date, aes(day, quotes_per_day)) +
  geom_line(aes(y = quotes_per_day))+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "quotes per Day", title = "Frequency of Tweet quotes mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
quotes_per_day
quotes_per_day_file_name <- paste0(here("quotes_per_day"),st,".png") 
ggsave(quotes_per_day_file_name)

replies_per_day <- ggplot(d8_tweets_by_date, aes(day, replies_per_day)) +
  geom_line(aes(y = replies_per_day))+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "replies per Day", title = "Frequency of Tweet replies mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
replies_per_day
replies_per_day_file_name <- paste0(here("replies_per_day"),st,".png") 
ggsave(replies_per_day_file_name)


```

# Syuzhet Sentiment Analysis with negation weighting method using sentimentr 

* by tweet

```{r, syuzhet method}
syuzhet_df <- readRDS("hashtag_delta8_tweets_and_user_info_jan_2020.rds")

syuzhet_df$text <- gsub("RT.*:", "", syuzhet_df$text)
syuzhet_df$text <- gsub("@.* ", "", syuzhet_df$text)
# sub out digits , punctuation
#Should digits be included in case of delta 8, delta 10? 
syuzhet_df_id <- syuzhet_df %>% 
   select(id,text)

sentiment_syuzhet <- syuzhet_df_id %>% 
  mutate(syuzhet_sentiment = get_sentiment(syuzhet_df_id$text))


# sentiment over time

sentiment_syuzhet_by_day <- syuzhet_df %>% 
  left_join(sentiment_syuzhet) %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day)) %>% 
  dplyr::group_by(day) %>% 
  dplyr::summarize(sentiment_avg_day = weighted.mean(syuzhet_sentiment))

#all datetimes sentiment dataframe
sentiment_syuzhet_tweets <- syuzhet_df %>% 
  left_join(sentiment_syuzhet) %>% 
  mutate(day = as.Date(created_at.x),
         month = month(day),
         year = year(day))


sentiment_per_day_plot <- ggplot(sentiment_syuzhet_by_day, aes(day, sentiment_avg_day)) +
  geom_line(aes(y = sentiment_avg_day))+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "Weighted Average Syuzhet Sentiment per Day", title = "Change in sentiment in Tweets mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
sentiment_per_day_plot
sentiment_per_day_plot_filename <- paste0(here("syuzhet_sentiment"),st,".png") 
ggsave(sentiment_per_day_plot_filename)

sentiment_all_plot <- ggplot(sentiment_syuzhet_tweets, aes(day, syuzhet_sentiment)) +
  geom_point(aes(y = syuzhet_sentiment), alpha = .05)+
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "January 1, 2020 to August 20, 2021", y = "Tweet Syuzhet Sentiments scores", title = "Change in sentiment in Tweets mentioning #delta8")+
  scale_x_date(date_breaks = "2 months", date_labels = "%b-%y")+
    theme_classic()
sentiment_all_plot

stats.default(sentiment_syuzhet$syuzhet_sentiment)
table1(~syuzhet_sentiment, data = sentiment_syuzhet)

negative_syuz <- sentiment_syuzhet %>% 
  arrange(-syuzhet_sentiment) %>% 
  head(n = 10) 
gt(negative_syuz) 

positive_syuz <- sentiment_syuzhet %>% 
  arrange(syuzhet_sentiment) %>% 
  head(n = 10)
gt(positive_syuz)
top10_pos_neg_syuz <- bind_rows(negative_syuz, positive_syuz) 


hist(sentiment_syuzhet$syuzhet_sentiment, main = "Distribution of Syuzhet Tweet-level Sentiment scores", xlab = "Szyuzhet Tweet-Level Sentiment")

sentiment_distribution <- ggplot(sentiment_syuzhet, aes(x=syuzhet_sentiment)) + 
  geom_histogram(binwidth = .5)+
  ggtitle("Distribution of Tweet-level Sentiment")

sentiment_distribution_filename <- paste0(here("sentiment_distribution"),st,".png") 
ggsave(sentiment_distribution_filename)



#investigate tweets within (-.5,0) score, seems most popular categorization

#test
```

